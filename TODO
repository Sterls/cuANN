09.12.2015
TODO:

        1. Create a "Test" function which takes as argumnt a "*.test" file.
           Run propagation for each input, collect sq_error and report back MSE.

        2. Create (de)serialisation for class ann: 
           Saving and Loading (Using BOOST) of a Trained network.

        3. Implement Resilient-Back-Propagation

        4. Create a more examples, using data-sets: `diabetes`,`gene`,`mushroom`,`soybean`,`thyroid`
           It is important that I have a Deep Network working BEFORE the end of December!!!

        5. It would be nice to be able to save on a file the actual recorded MSE's.

        6. OPTIMIZE: the `trainer` class has serious problems: too many copies and allocations.
                     What I need is a flat (and more elegant) way of fw_propagating, which is spamming `cudaMemcpy` and `cudaMemaloc`
                     In order to do this, I need to rethink my `trainer::fw_propagate` seriously.

                     A CRAZY IDEA, would be to ALLOCATE before-hand ALL Pattern-trainer_data.
                     In this case, I would allocate as many training patterns ONCE.
                     Then for each Epoch Iteration, I would re-use the same Pattern-Trainer_Data.
                     This will use ALOT of Memory, but will avoid constant re-allocations and copying.
                     It may be feasible with normal training sets, but may become a problem with large ones.

NOTE: Profiling using nvprof was revealing: most time is spent allocating and copying memory.
      This needs to be addressed and fixed.
