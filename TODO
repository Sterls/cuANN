10.12.2015
TODO: [Version 0.2]

        NOW. Create more samplex using data-sets: `robot`, `soybean`, `thyroid`

        NEXT. Save on a file the actual recorded MSE's. Save on a local member variable, and save on file if needed.


TODO: [Version 0.3]

        1. Average Cross-Entropy Error.

        2. Squared MSE.
        
        3. Implement Resilient-Back-Propagation.
           Once complete, rename `train` to `train_backprop` and create a new `train_rprop` method.
           Then, parametrise the `epoch` method to take as param either an ENUM, or a functor to the batch training method.

        4. OPTIMIZE: the `trainer` class has serious problems: too many copies and allocations.
                     What I need is a flat (and more elegant) way of fw_propagating, which is spamming `cudaMemcpy` and `cudaMemaloc`
                     In order to do this, I need to rethink my `trainer::fw_propagate` seriously.

         A CRAZY IDEA, would be to ALLOCATE before-hand ALL Pattern-trainer_data.
         In this case, I would allocate as many training patterns ONCE.
         Then for each Epoch Iteration, I would re-use the same Pattern-Trainer_Data.
         This will use ALOT of Memory, but will avoid constant re-allocations and copying.
         It may be feasible with normal training sets, but may become a problem with large ones.

       5. Enable Output Regression (Soft-Max) to properly scale the output.


NOTE: Profiling using nvprof was revealing: most time is spent allocating and copying memory.
      This needs to be addressed and fixed.
